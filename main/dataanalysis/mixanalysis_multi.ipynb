{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmixdata_sim=[]\\nfor i in range(len(interdata_sim)):\\n    mixdata_sim=mixdata_sim+[biodata_sim[i][0:10]+biodata_sim[i]]\\nprint(mixdata_sim[0])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from simdata import get_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "######################################\n",
    "# Getting data from data sources\n",
    "######################################\n",
    "#=========================\n",
    "#get data from csv files\n",
    "#=========================\n",
    "mixdata_sim_filedir='../resources/gamedata/mixdata_sim.csv'\n",
    "mixdata_sim=get_mldata(mixdata_sim_filedir)\n",
    "\n",
    "\n",
    "inerdata_sim_filedir='../resources/gamedata/interdata_sim.csv'\n",
    "biodata_sim_filedir='../resources/gamedata/biodata_sim.csv'\n",
    "interdata_sim=get_mldata(inerdata_sim_filedir)\n",
    "biodata_sim=get_mldata(biodata_sim_filedir)\n",
    "'''\n",
    "mixdata_sim=[]\n",
    "for i in range(len(interdata_sim)):\n",
    "    mixdata_sim=mixdata_sim+[biodata_sim[i][0:10]+biodata_sim[i]]\n",
    "print(mixdata_sim[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Clustering based on interdata\n",
    "############################################\n",
    "#===========================\n",
    "# get data for clustering\n",
    "#===========================\n",
    "features_km=[]\n",
    "for i in mixdata_sim:\n",
    "    features_km=features_km+[i[0:4]]\n",
    "features_km_np=np.array(features_km)\n",
    "features_km_np = (features_km_np - features_km_np.mean(axis=0)) / features_km_np.std(axis=0)\n",
    "\n",
    "#=================================\n",
    "# cluster interdata by kmeans\n",
    "#=================================\n",
    "cluster_inter=KMeans(n_clusters=2,random_state=0).fit(features_km_np)\n",
    "\n",
    "\n",
    "#============================\n",
    "#labels from cluster\n",
    "#============================\n",
    "labels_pre_clu=cluster_inter.labels_\n",
    "labels_pre_clu_centers=cluster_inter.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Classify samples\n",
    "#######################################\n",
    "#================================\n",
    "# data for classifying analysis\n",
    "#================================\n",
    "labels_cla=[]\n",
    "for i in labels_pre_clu:\n",
    "    if i==0:\n",
    "        #labels_cla=labels_cla+[labels_pre_clu_centers[0]]\n",
    "        labels_cla=labels_cla+[[0.]]\n",
    "    else:\n",
    "        #labels_cla=labels_cla+[labels_pre_clu_centers[1]]\n",
    "        labels_cla=labels_cla+[[1.]]\n",
    "\n",
    "labels_cla_np = np.array(labels_cla)\n",
    "\n",
    "features_cla=[]\n",
    "for i in mixdata_sim:\n",
    "    features_cla=features_cla+[i[4:14]]\n",
    "\n",
    "features_cla_np = np.array(features_cla)\n",
    "features_cla_np = (features_cla_np - features_cla_np.mean(axis=0))/features_cla_np.std(axis=0)# follow natural distribution\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features_cla_np, labels_cla_np, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cancui/anaconda3/envs/abiba_analysis_v2/lib/python3.5/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#============================\n",
    "#classify biodata by SVM\n",
    "#============================\n",
    "clf_svm=svm.SVC(C=1)\n",
    "clf_svm.fit(train_features,train_labels)\n",
    "labels_pre_svm=clf_svm.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cancui/anaconda3/envs/abiba_analysis_v2/lib/python3.5/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#===============================\n",
    "#classify biodata by GaussianNB\n",
    "#===============================\n",
    "clf_gnb=GaussianNB()\n",
    "clf_gnb.fit(train_features,train_labels)\n",
    "labels_pre_gnb=clf_gnb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable w1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-1-d7c5a1f44da4>\", line 115, in <module>\n    weight1=tf.get_variable('w1',[num_features,num_hidden_units_layer1],tf.float32)\n  File \"/Users/cancui/anaconda3/envs/abiba_analysis_v2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/cancui/anaconda3/envs/abiba_analysis_v2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-540303a5fd09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#hidden layer1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mweight1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_hidden_units_layer1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mbiase1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_hidden_units_layer1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#l1_output=tf.sigmoid(tf.matmul(NN_input,weight1)+biase1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/anaconda3/envs/abiba_analysis/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/workspace/anaconda3/envs/abiba_analysis/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/workspace/anaconda3/envs/abiba_analysis/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/workspace/anaconda3/envs/abiba_analysis/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/anaconda3/envs/abiba_analysis/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable w1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-1-d7c5a1f44da4>\", line 115, in <module>\n    weight1=tf.get_variable('w1',[num_features,num_hidden_units_layer1],tf.float32)\n  File \"/Users/cancui/anaconda3/envs/abiba_analysis_v2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/cancui/anaconda3/envs/abiba_analysis_v2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#====================================\n",
    "#classify biodata by Neural Network\n",
    "#====================================\n",
    "#==========================\n",
    "# Set Network Structure\n",
    "#==========================\n",
    "#variables for network\n",
    "num_features=10\n",
    "num_hidden_units_layer1=100\n",
    "num_hidden_units_layer2=100\n",
    "num_hidden_units_layer3=100\n",
    "num_output_units=1\n",
    "\n",
    "#input/output information\n",
    "NN_input=tf.placeholder(tf.float32,[None, num_features],name=\"NN_input\")\n",
    "NN_output=tf.placeholder(tf.float32,[None, num_output_units],name=\"NN_output\")\n",
    "\n",
    "\n",
    "#hidden layer1\n",
    "weight1=tf.get_variable('w1',[num_features,num_hidden_units_layer1],tf.float32)\n",
    "biase1=tf.get_variable('b1',[num_hidden_units_layer1],tf.float32)\n",
    "#l1_output=tf.sigmoid(tf.matmul(NN_input,weight1)+biase1)\n",
    "l1_output = tf.nn.relu(tf.matmul(NN_input, weight1)+biase1)\n",
    "\n",
    "#hidden layer2\n",
    "weight2=tf.get_variable('w2',[num_features,num_hidden_units_layer2],tf.float32)\n",
    "biase2=tf.get_variable('b2',[num_hidden_units_layer2],tf.float32)\n",
    "#l2_output=tf.sigmoid(tf.matmul(NN_input,weight1)+biase1)\n",
    "l2_output = tf.nn.relu(tf.matmul(NN_input, weight2)+biase2)\n",
    "\n",
    "#hidden layer2\n",
    "weight3=tf.get_variable('w3',[num_features,num_hidden_units_layer3],tf.float32)\n",
    "biase3=tf.get_variable('b3',[num_hidden_units_layer3],tf.float32)\n",
    "#l3_output=tf.sigmoid(tf.matmul(NN_input,weight1)+biase1)\n",
    "l3_output = tf.nn.relu(tf.matmul(NN_input, weight3)+biase3)\n",
    "\n",
    "#output layer\n",
    "weight_out=tf.get_variable('w_out',[num_hidden_units_layer3,num_output_units],tf.float32)\n",
    "biase_out=tf.get_variable('b_out',[num_output_units],tf.float32)\n",
    "\n",
    "output_logits=tf.matmul(l3_output,weight_out)+biase_out\n",
    "output = tf.nn.sigmoid(output_logits)\n",
    "\n",
    "#train methods\n",
    "#loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=NN_output,logits=output_logits)) #to be used for [[0,1,0],[1,0,0]...[]]\n",
    "#loss = tf.nn.l2_loss(output_logits - NN_output)\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=output_logits, labels=NN_output)\n",
    "\n",
    "l1_regularizer=tf.contrib.layers.l1_regularizer(scale=0.1,scope=None)\n",
    "loss_fun=loss+tf.contrib.layers.apply_regularization(l1_regularizer,[weight3])\n",
    "\n",
    "# train method\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_fun)\n",
    "#train_step=tf.train.GradientDescentOptimizer(0.005).minimize(loss_fun)\n",
    "\n",
    "\n",
    "#===========================\n",
    "# Train Network\n",
    "#===========================\n",
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "#init_g=tf.global_variables_initializer()\n",
    "#init_l=tf.local_variables_initializer()\n",
    "\n",
    "for i in range(20000):\n",
    "    batch_xs,batch_ys=(train_features,train_labels)\n",
    "    c_loss, _ = sess.run([loss, train_step],feed_dict={NN_input:batch_xs,NN_output:batch_ys})\n",
    "    #print(c_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================\n",
    "# Evaluation\n",
    "#=============================\n",
    "\n",
    "correct_prediction = tf.equal(tf.round(NN_output), tf.round(output))\n",
    "prediction_accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "acc_nn = sess.run(prediction_accuracy,feed_dict={NN_input:test_features, NN_output: test_labels})\n",
    "print(\"acc_nn:\",acc_nn)\n",
    "\n",
    "acc_gnb=accuracy_score(test_labels,labels_pre_gnb)\n",
    "print(\"acc_gnb:\",acc_gnb)\n",
    "\n",
    "acc_svm=accuracy_score(test_labels,labels_pre_svm)\n",
    "print(\"acc_svm:\",acc_svm)\n",
    "\n",
    "acc_dic={'classifier_name':[\"nn\",\"svm\",\"gnb\"],'classifier_acc':[acc_nn,acc_svm,acc_gnb]}\n",
    "highest_acc_index=np.argmax(acc_dic)\n",
    "print(\"the best classifier is:\",acc_dic[\"classifier_name\"][highest_acc_index])\n",
    "#=================\n",
    "# analysis\n",
    "#=================\n",
    "biolabels=[]\n",
    "interlabels=[]\n",
    "for i in range(len(biodata_sim)):\n",
    "    biolabels=biolabels+[biodata_sim[i][10]]\n",
    "\n",
    "biolabels_np=np.array(biolabels)\n",
    "\n",
    "cluster_labels=[]\n",
    "classify_labels=labels_pre_svm\n",
    "for i in range(len(labels_pre_clu)):\n",
    "    if labels_pre_clu[i]==1:\n",
    "        cluster_labels=cluster_labels+[0]\n",
    "    else:\n",
    "        cluster_labels = cluster_labels + [1]\n",
    "\n",
    "acc_clu_bio=accuracy_score(biolabels,cluster_labels)\n",
    "print(\"acc_clu_bio:\",acc_clu_bio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}