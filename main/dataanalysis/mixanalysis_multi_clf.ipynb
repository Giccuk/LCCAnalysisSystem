{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from simdata import get_mldata\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Getting data from data sources\n",
    "######################################\n",
    "#=========================\n",
    "#get data from csv files\n",
    "#=========================\n",
    "mixdata_sim_filedir='../resources/gamedata/mixdata_sim.csv'\n",
    "mixdata_sim=get_mldata(mixdata_sim_filedir)\n",
    "\n",
    "\n",
    "inerdata_sim_filedir='../resources/gamedata/interdata_sim.csv'\n",
    "biodata_sim_filedir='../resources/gamedata/biodata_sim.csv'\n",
    "interdata_sim=get_mldata(inerdata_sim_filedir)\n",
    "biodata_sim=get_mldata(biodata_sim_filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Clustering based on interdata\n",
    "############################################\n",
    "#===========================\n",
    "# get data for clustering\n",
    "#===========================\n",
    "features_km=[]\n",
    "for i in mixdata_sim:\n",
    "    features_km=features_km+[i[0:4]]\n",
    "features_km_np=np.array(features_km)\n",
    "features_km_np = (features_km_np - features_km_np.mean(axis=0)) / features_km_np.std(axis=0)\n",
    "\n",
    "#=================================\n",
    "# cluster interdata by kmeans\n",
    "#=================================\n",
    "cluster_inter=KMeans(n_clusters=2,random_state=0).fit(features_km_np)\n",
    "\n",
    "\n",
    "#============================\n",
    "#labels from cluster\n",
    "#============================\n",
    "labels_pre_clu=cluster_inter.labels_\n",
    "labels_pre_clu_centers=cluster_inter.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Classify samples\n",
    "#######################################\n",
    "#================================\n",
    "# data for classifying analysis\n",
    "#================================\n",
    "labels_cla=[]\n",
    "for i in labels_pre_clu:\n",
    "    if i==0:\n",
    "        #labels_cla=labels_cla+[labels_pre_clu_centers[0]]\n",
    "        labels_cla=labels_cla+[[0.]]\n",
    "    else:\n",
    "        #labels_cla=labels_cla+[labels_pre_clu_centers[1]]\n",
    "        labels_cla=labels_cla+[[1.]]\n",
    "\n",
    "labels_cla_np = np.array(labels_cla)\n",
    "\n",
    "features_cla=[]\n",
    "for i in mixdata_sim:\n",
    "    features_cla=features_cla+[i[4:14]]\n",
    "\n",
    "features_cla_np = np.array(features_cla)\n",
    "features_cla_np = (features_cla_np - features_cla_np.mean(axis=0))/features_cla_np.std(axis=0)# follow natural distribution\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features_cla_np, labels_cla_np, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cancui/anaconda3/envs/abiba_analysis_v2/lib/python3.5/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#============================\n",
    "#classify biodata by SVM\n",
    "#============================\n",
    "clf_svm=svm.SVC(C=1)\n",
    "clf_svm.fit(train_features,train_labels)\n",
    "labels_pre_svm=clf_svm.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cancui/anaconda3/envs/abiba_analysis_v2/lib/python3.5/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#===============================\n",
    "#classify biodata by GaussianNB\n",
    "#===============================\n",
    "clf_gnb=GaussianNB()\n",
    "clf_gnb.fit(train_features,train_labels)\n",
    "labels_pre_gnb=clf_gnb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================\n",
    "#classify biodata by Neural Network\n",
    "#====================================\n",
    "#==========================\n",
    "# Set Network Structure\n",
    "#==========================\n",
    "#variables for network\n",
    "num_features=10\n",
    "num_hidden_units_layer1=100\n",
    "num_hidden_units_layer2=100\n",
    "num_hidden_units_layer3=100\n",
    "num_output_units=1\n",
    "\n",
    "#input/output information\n",
    "NN_input=tf.placeholder(tf.float32,[None, num_features],name=\"NN_input\")\n",
    "NN_output=tf.placeholder(tf.float32,[None, num_output_units],name=\"NN_output\")\n",
    "\n",
    "\n",
    "#hidden layer1\n",
    "weight1=tf.get_variable('w1',[num_features,num_hidden_units_layer1],tf.float32)\n",
    "biase1=tf.get_variable('b1',[num_hidden_units_layer1],tf.float32)\n",
    "#l1_output=tf.sigmoid(tf.matmul(NN_input,weight1)+biase1)\n",
    "l1_output = tf.nn.relu(tf.matmul(NN_input, weight1)+biase1)\n",
    "\n",
    "#hidden layer2\n",
    "weight2=tf.get_variable('w2',[num_features,num_hidden_units_layer2],tf.float32)\n",
    "biase2=tf.get_variable('b2',[num_hidden_units_layer2],tf.float32)\n",
    "#l2_output=tf.sigmoid(tf.matmul(NN_input,weight1)+biase1)\n",
    "l2_output = tf.nn.relu(tf.matmul(NN_input, weight2)+biase2)\n",
    "\n",
    "#hidden layer2\n",
    "weight3=tf.get_variable('w3',[num_features,num_hidden_units_layer3],tf.float32)\n",
    "biase3=tf.get_variable('b3',[num_hidden_units_layer3],tf.float32)\n",
    "#l3_output=tf.sigmoid(tf.matmul(NN_input,weight1)+biase1)\n",
    "l3_output = tf.nn.relu(tf.matmul(NN_input, weight3)+biase3)\n",
    "\n",
    "#output layer\n",
    "weight_out=tf.get_variable('w_out',[num_hidden_units_layer3,num_output_units],tf.float32)\n",
    "biase_out=tf.get_variable('b_out',[num_output_units],tf.float32)\n",
    "\n",
    "output_logits=tf.matmul(l3_output,weight_out)+biase_out\n",
    "output = tf.nn.sigmoid(output_logits)\n",
    "\n",
    "#train methods\n",
    "#loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=NN_output,logits=output_logits)) #to be used for [[0,1,0],[1,0,0]...[]]\n",
    "#loss = tf.nn.l2_loss(output_logits - NN_output)\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=output_logits, labels=NN_output)\n",
    "\n",
    "l1_regularizer=tf.contrib.layers.l1_regularizer(scale=0.1,scope=None)\n",
    "loss_fun=loss+tf.contrib.layers.apply_regularization(l1_regularizer,[weight3])\n",
    "\n",
    "# train method\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_fun)\n",
    "#train_step=tf.train.GradientDescentOptimizer(0.005).minimize(loss_fun)\n",
    "\n",
    "\n",
    "#===========================\n",
    "# Train Network\n",
    "#===========================\n",
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "#init_g=tf.global_variables_initializer()\n",
    "#init_l=tf.local_variables_initializer()\n",
    "\n",
    "for i in range(20000):\n",
    "    batch_xs,batch_ys=(train_features,train_labels)\n",
    "    c_loss, _ = sess.run([loss, train_step],feed_dict={NN_input:batch_xs,NN_output:batch_ys})\n",
    "    #print(c_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_nn: 0.708333\n",
      "acc_gnb: 0.578125\n",
      "acc_svm: 0.708333333333\n",
      "the best classifier is: nn\n",
      "acc_clu_bio: 1.0\n"
     ]
    }
   ],
   "source": [
    "#=============================\n",
    "# Evaluation\n",
    "#=============================\n",
    "\n",
    "correct_prediction = tf.equal(tf.round(NN_output), tf.round(output))\n",
    "prediction_accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "acc_nn = sess.run(prediction_accuracy,feed_dict={NN_input:test_features, NN_output: test_labels})\n",
    "print(\"acc_nn:\",acc_nn)\n",
    "\n",
    "acc_gnb=accuracy_score(test_labels,labels_pre_gnb)\n",
    "print(\"acc_gnb:\",acc_gnb)\n",
    "\n",
    "acc_svm=accuracy_score(test_labels,labels_pre_svm)\n",
    "print(\"acc_svm:\",acc_svm)\n",
    "\n",
    "acc_dic={'classifier_name':[\"nn\",\"svm\",\"gnb\"],'classifier_acc':[acc_nn,acc_svm,acc_gnb]}\n",
    "highest_acc_index=np.argmax(acc_dic)\n",
    "print(\"the best classifier is:\",acc_dic[\"classifier_name\"][highest_acc_index])\n",
    "#=================\n",
    "# analysis\n",
    "#=================\n",
    "biolabels=[]\n",
    "interlabels=[]\n",
    "for i in range(len(biodata_sim)):\n",
    "    biolabels=biolabels+[biodata_sim[i][10]]\n",
    "\n",
    "biolabels_np=np.array(biolabels)\n",
    "\n",
    "cluster_labels=[]\n",
    "classify_labels=labels_pre_svm\n",
    "for i in range(len(labels_pre_clu)):\n",
    "    if labels_pre_clu[i]==1:\n",
    "        cluster_labels=cluster_labels+[0]\n",
    "    else:\n",
    "        cluster_labels = cluster_labels + [1]\n",
    "\n",
    "acc_clu_bio=accuracy_score(biolabels,cluster_labels)\n",
    "print(\"acc_clu_bio:\",acc_clu_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
