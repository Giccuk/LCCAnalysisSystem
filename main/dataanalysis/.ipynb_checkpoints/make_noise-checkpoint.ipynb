{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, re, csv\n",
    "import numpy as np\n",
    "import math\n",
    "from simdata import *\n",
    "# for pre-processing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# for kmeans\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "# for svm\n",
    "from sklearn import metrics,svm\n",
    "#for evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "# for visulization\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#==============================================\n",
    "# make synthic data\n",
    "#==============================================\n",
    "sample_num=1000\n",
    "d_ratio=0.5\n",
    "dsample_num=math.ceil(sample_num*d_ratio)\n",
    "ndsample_num=sample_num-dsample_num\n",
    "\n",
    "#depressive:  OR:[0.6,0,9], CR:[0.4,1.0], IR:[0.5,0.7], RR:[0.4,0.6]\n",
    "#no_depessive: OR:[0.3,0.5], CR:[0.6,1.0], IR:[0.1,0.3], RR:[0.1,0.3]\n",
    "bhf_range={'or':[6,10,3,6],\"cr\":[4,11,6,11],\"ir\":[5,8,1,4],\"rr\":[4,7,1,4]}\n",
    "\n",
    "syn_bhdata=make_syn_bhdata(sample_num,d_ratio,bhf_range)\n",
    "\n",
    "syn_bhf=syn_bhdata[\"syn_bhf\"]\n",
    "syn_prel=syn_bhdata[\"syn_pre_bhl\"]\n",
    "\n",
    "nbhf_num=10\n",
    "syn_nbhf=make_syn_nbhf(nbhf_num,sample_num)\n",
    "\n",
    "synf=np.insert(syn_nbhf,[0],syn_bhf,axis=1)\n",
    "syndata=np.insert(syn_prel.reshape(1000,1),[0],synf,axis=1)\n",
    "\n",
    "syndata_filedir=\"../resources/gamedata/syndata.csv\"\n",
    "create_csvfile(syndata,syndata_filedir)\n",
    "\"\"\"\n",
    "\n",
    "#=================================\n",
    "# get mixdata from csv files:\n",
    "# 1. ipld mixdata\n",
    "# 2. syn mixdata\n",
    "# mixdata: [[interdata,biodata,prelabel],...,[]]\n",
    "#================================\n",
    "syndata_filedir=\"../resources/gamedata/syndata.csv\"\n",
    "ipldmixdata_sim_filedir='../resources/gamedata/ipldmixdata_sim.csv'\n",
    "\n",
    "#mixdata=np.asarray(get_mldata(ipldmixdata_sim_filedir),dtype=float)\n",
    "mixdata=np.asarray(get_mldata(syndata_filedir),dtype=float)\n",
    "\n",
    "#==============================================\n",
    "#prepare features/labels for k-means\n",
    "#==============================================\n",
    "mixdata_feature=mixdata[:,0:mixdata[0].shape[0]-1]\n",
    "mixdata_scaler = StandardScaler()\n",
    "mixdata_feature_nd=mixdata_scaler.fit_transform(mixdata_feature)\n",
    "\n",
    "mixdata_prelabel=mixdata[:,mixdata[0].shape[0]-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# aim: tranformed new data to normal distribution followed by old data\n",
    "# in: new_data 1*n, \n",
    "#     olddata_scaler: data scaler\n",
    "# out: transformed data\n",
    "#-----------------------------------------------------\n",
    "def trans_data(new_data,olddata_scaler):\n",
    "    transformed_data=olddata_scaler.transform(new_data.reshape(1, -1))\n",
    "    return transformed_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# aim: build km cluster and svm classifier from data\n",
    "# in: bhdata,nbhdata;\n",
    "#     subset_num(number of subsets)\n",
    "# out: dict={\"cluer\":cluster_km,\"clfer\":classifier_svm}\n",
    "#-----------------------------------------------------\n",
    "def build_analyser(bhdata,nbhdata,subset_num):\n",
    "    #Cluster from kmeans\n",
    "    cluster_km = KMeans(n_clusters=subset_num, random_state=0).fit(bhdata)\n",
    "    #Classifier from SVM\n",
    "    cluster_result=cluster_km.labels_\n",
    "    '''clf_train_feature,\\\n",
    "    clf_test_feature,\\\n",
    "    clf_train_label,\\\n",
    "    clf_test_label = train_test_split(nbhdata,cluster_result, test_size=test_ratio)'''\n",
    "    # build classifier\n",
    "    classifier_svm=svm.SVC(C=1).fit(nbhdata,cluster_result)\n",
    "    return {\"clu\":cluster_km,\"clf\":classifier_svm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtrain_feature,\\\n",
    "mixtest_feature,\\\n",
    "mixtrain_label,\\\n",
    "mixtest_label = train_test_split(mixdata_feature_nd,mixdata_prelabel, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "# Build processors from original\n",
    "#==================================\n",
    "processors_ori=build_analyser(mixtrain_feature[:,0:4],mixtrain_feature[:,5:],2)\n",
    "clfer_ori=processors_ori[\"clf\"]\n",
    "cluer_ori=processors_ori['clu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity test:  1.0\n"
     ]
    }
   ],
   "source": [
    "clu_result=cluer_ori.predict(mixtest_feature[:,0:4])\n",
    "print(\"similarity test: \",metrics.adjusted_rand_score(clu_result,mixtest_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "# make noise\n",
    "#==================================\n",
    "#depressive:  OR:[0.6,0,9], CR:[0.4,1.0], IR:[0.5,0.7], RR:[0.4,0.6]\n",
    "#no_depessive: OR:[0.3,0.5], CR:[0.6,1.0], IR:[0.1,0.3], RR:[0.1,0.3]\n",
    "\n",
    "# make uniform noise(int) for behaviourdata\n",
    "num_sample=mixdata_feature.shape[0]\n",
    "\n",
    "bhf_noise=np.random.randint(0,11,num_sample).reshape(num_sample,1)\n",
    "for i in range(3):\n",
    "    noise_v=np.random.randint(0,11,num_sample).reshape(num_sample,1)\n",
    "    bhf_noise = np.insert(bhf_noise,[1],noise_v,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaler = StandardScaler()\n",
    "bhf_noise_nd=data_scaler.fit_transform(bhf_noise)\n",
    "all_processors_noise=build_analyser(bhf_noise_nd,mixdata_feature_nd[:,5:],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfer_noise=all_processors_noise[\"clf\"]\n",
    "cluer_noise=all_processors_noise['clu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_result_ori=cluer_ori.predict(mixdata_feature_nd[:,0:4])\n",
    "\n",
    "\n",
    "\n",
    "one_num=0\n",
    "for i in mixdata_prelabel:\n",
    "    if i==1.0:\n",
    "        one_num=one_num+1\n",
    "\n",
    "print(one_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new classifier\n",
    "all_processors=build_analyser(noised_bhf,mixdata_feature_nd[:,5:],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data=np.arange(0,noisydata.size)\n",
    "plt.plot(num_data,noisydata,'ro-',label='Noisy Data')\n",
    "plt.plot(rawdata,'bo-',label='Raw Data')\n",
    "plt.legend() # 展示图例\n",
    "plt.xlabel('Offer type') # 给 x 轴添加标签\n",
    "plt.ylabel('Offer Value') # 给 y 轴添加标签\n",
    "plt.title('Uniform Noise Analysis') # 添加图形标题\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================\n",
    "# add noise \n",
    "#=========================\n",
    "# name:get_uninoisylabel\n",
    "# fun: add uniform noise in to data\n",
    "# in: rawdata(np.array, NOT normalized), valuerange([low,high+1], range of noise)\n",
    "# out: noisydata(np.array)\n",
    "def get_uninoisylabel(rawlabel,noiselevel):\n",
    "    noisenum=int(noiselevel/100*rawlabel.size)\n",
    "    uninoise=np.random.randint(0,1,size=noisenum)\n",
    "    label_uninoise=np.append(uninoise,rawlabel[noisenum:])\n",
    "    return label_uninoise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiselevel=40\n",
    "#temp_tl=train_labels[0:10]\n",
    "train_labels_uninoise=get_uninoisylabel(train_labels,noiselevel) \n",
    "#============================\n",
    "#classify interata by SVM\n",
    "#============================\n",
    "\n",
    "clf_svm=svm.SVC(C=1)\n",
    "clf_svm.fit(train_features,train_labels_uninoise)\n",
    "labels_pre_svm=clf_svm.predict(test_features)\n",
    "#============================\n",
    "# Evaluation\n",
    "#============================\n",
    "\n",
    "acc_svm=accuracy_score(test_labels,labels_pre_svm)\n",
    "print(\"acc_svm:\",acc_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count, bins, ignored = plt.hist(s, 2,normed=True)\n",
    "plt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\n",
    "plt.show()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhdata=10*3, nbhdata=10*2 \n",
    "bhdata=np.append(np.arange(15,dtype=float),np.arange(20,35)).reshape(10,3)\n",
    "nbhdata=np.append(np.arange(10,11,0.1),np.arange(20,23,0.3)).reshape(10,2)\n",
    "data_indx=np.arange(1,bhdata.shape[0]+1).reshape(bhdata.shape[0],1)\n",
    "\n",
    "#alldata=[ [indx,bhdata,nhbdata],...,[] ], 10*6\n",
    "alldata=np.insert(np.insert(bhdata,[3],nbhdata,axis=1),[0],data_indx,axis=1)\n",
    "\n",
    "print(alldata[:,1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(bhdata)\n",
    "kresult= kmeans.predict(bhdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.uniform(-1,0,1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
